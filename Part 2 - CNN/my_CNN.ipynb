{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Christine's Code: CNN Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# if you're running into issues with images at the compile step, pip install pillow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras now lives under TF\n",
    "a = tf.keras.preprocessing.image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4,000 images of dogs and 4,000 images of cats\n",
    "# Total of 8,000 images\n",
    "\n",
    "# Juypter notebook - run code via your machine with VSCode\n",
    "# Google Colab will fail for highly intensive code that requires large datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to deploy so it can differentiate between 2 different images\n",
    "# Dog and Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to apply transformations to avoid overfitting\n",
    "# high accuracy on training set, low accuracy in test set\n",
    "# Why we do this - avoid overfitting\n",
    "\n",
    "# Image Augmentation\n",
    "# Your CNN does not overlearn\n",
    "\n",
    "# Example - horizontal flips, zooms, rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create Preprocessor for Training Set\n",
    "# Create instance, and create training class generator\n",
    "training_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255, # feature scaling\n",
    "    shear_range = 0.2, # \n",
    "    zoom_range = 0.2, # Zoom into the picture\n",
    "    horizontal_flip = True # flip horizontally\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the training set\n",
    "# Flow from directory - tell it where the dataset for training is coming from\n",
    "training_set = training_datagen.flow_from_directory('/Users/christine/VSCode/deeplearningcourse_remote/deeplearningcourse/Part 2 - CNN/training_set',\n",
    "                                                 target_size = (64, 64), # affects resizing\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary') # this is a binary outcome, cat or dog\n",
    "\n",
    "# Notice the folders are labelled dog and cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create instance, and create test class generator\n",
    "\n",
    "# DO NOT APPLY THE TRANSFORMATIONS FOR TEST-GEN EXCEPT FOR SCALING\n",
    "\n",
    "# Test image - you do not want to apply transformations, to prevent over-learning\n",
    "# i.e. not applying fit_transform() to test set, you only transform()\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('/Users/christine/VSCode/deeplearningcourse_remote/deeplearningcourse/Part 2 - CNN/test_set',\n",
    "                                            target_size = (64, 64), # must be the same size and parameters here\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "# 2000 images for test set\n",
    "# 8000 images for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Initialize CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to initialize the CNN:\n",
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1 - Convolution Layer = Convolution + RELU + Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/deeplearningcourse_remote/deeplearningcourse/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Apply Convolution to have the Convolution layer:\n",
    "# Three important parameters:\n",
    "# Filter or feature detectors or kernels \n",
    "# Kernel Size = size of the rows and columns (square) of the size the square scanning around. The \n",
    "# Activation = always RELU function\n",
    "# Input Shape = you have to specify the input shape, 3 dimensions, RGB, since it was resized, we need to do 64 by 64 by 3\n",
    "# 64 by 64 is the image shape, then \"3\" is the RBG coloring. If it was singular color, you'd do 1.\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling - second layer to consider things like spatial invariance\n",
    "# Pool Size = 2 (it's square so 2 by 2)\n",
    "# Strides = How many pixels the square moves around\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size= 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 2nd Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because this 2nd layer is connecting CL1 to CL2, you don't need the input shape parameter again:\n",
    "\n",
    "# Step 1 - Create the layer itself, and specify RELU\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Step 2 - Max Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size= 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Flatten CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take CNN method under tf > keras > layers > Flatten() function\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT YOURSELF:\n",
    "\n",
    "# Create the 2nd last layer before output:\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation= \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the output layer:\n",
    "cnn.add(tf.keras.layers.Dense(units= 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the CNN\n",
    "cnn.compile(optimizer= \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Adam = Stochastic Gradient Descent\n",
    "# Loss = Binary Cross Entropy - for binary classification problems\n",
    "# Metrics = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/VSCode/deeplearningcourse_remote/deeplearningcourse/.venv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 48ms/step - accuracy: 0.5374 - loss: 0.7110 - val_accuracy: 0.6160 - val_loss: 0.6429\n",
      "Epoch 2/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - accuracy: 0.6375 - loss: 0.6376 - val_accuracy: 0.7095 - val_loss: 0.5769\n",
      "Epoch 3/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - accuracy: 0.6912 - loss: 0.5814 - val_accuracy: 0.7280 - val_loss: 0.5457\n",
      "Epoch 4/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.7171 - loss: 0.5453 - val_accuracy: 0.7570 - val_loss: 0.5047\n",
      "Epoch 5/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7482 - loss: 0.5095 - val_accuracy: 0.7705 - val_loss: 0.5006\n",
      "Epoch 6/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7554 - loss: 0.5004 - val_accuracy: 0.7775 - val_loss: 0.4724\n",
      "Epoch 7/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.7693 - loss: 0.4821 - val_accuracy: 0.7795 - val_loss: 0.4631\n",
      "Epoch 8/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7692 - loss: 0.4659 - val_accuracy: 0.7850 - val_loss: 0.4594\n",
      "Epoch 9/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.7841 - loss: 0.4552 - val_accuracy: 0.7655 - val_loss: 0.4855\n",
      "Epoch 10/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.7942 - loss: 0.4369 - val_accuracy: 0.7905 - val_loss: 0.4562\n",
      "Epoch 11/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7951 - loss: 0.4322 - val_accuracy: 0.8025 - val_loss: 0.4343\n",
      "Epoch 12/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - accuracy: 0.8022 - loss: 0.4235 - val_accuracy: 0.7915 - val_loss: 0.4610\n",
      "Epoch 13/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - accuracy: 0.8097 - loss: 0.4140 - val_accuracy: 0.7900 - val_loss: 0.4449\n",
      "Epoch 14/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - accuracy: 0.8184 - loss: 0.3952 - val_accuracy: 0.7935 - val_loss: 0.4494\n",
      "Epoch 15/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 45ms/step - accuracy: 0.8287 - loss: 0.3816 - val_accuracy: 0.7910 - val_loss: 0.4591\n",
      "Epoch 16/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 45ms/step - accuracy: 0.8303 - loss: 0.3745 - val_accuracy: 0.8065 - val_loss: 0.4365\n",
      "Epoch 17/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.8334 - loss: 0.3613 - val_accuracy: 0.8065 - val_loss: 0.4431\n",
      "Epoch 18/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - accuracy: 0.8392 - loss: 0.3590 - val_accuracy: 0.8125 - val_loss: 0.4277\n",
      "Epoch 19/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8419 - loss: 0.3498 - val_accuracy: 0.7995 - val_loss: 0.4589\n",
      "Epoch 20/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - accuracy: 0.8436 - loss: 0.3488 - val_accuracy: 0.8060 - val_loss: 0.4393\n",
      "Epoch 21/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.8531 - loss: 0.3267 - val_accuracy: 0.8075 - val_loss: 0.4488\n",
      "Epoch 22/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.8573 - loss: 0.3222 - val_accuracy: 0.7955 - val_loss: 0.5010\n",
      "Epoch 23/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.8551 - loss: 0.3242 - val_accuracy: 0.8160 - val_loss: 0.4438\n",
      "Epoch 24/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.8627 - loss: 0.3103 - val_accuracy: 0.7895 - val_loss: 0.5019\n",
      "Epoch 25/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.8727 - loss: 0.2979 - val_accuracy: 0.8065 - val_loss: 0.4696\n",
      "Epoch 26/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 53ms/step - accuracy: 0.8639 - loss: 0.3058 - val_accuracy: 0.8160 - val_loss: 0.4456\n",
      "Epoch 27/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - accuracy: 0.8795 - loss: 0.2757 - val_accuracy: 0.8115 - val_loss: 0.4498\n",
      "Epoch 28/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - accuracy: 0.8840 - loss: 0.2711 - val_accuracy: 0.8130 - val_loss: 0.4615\n",
      "Epoch 29/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 61ms/step - accuracy: 0.8858 - loss: 0.2748 - val_accuracy: 0.8025 - val_loss: 0.5403\n",
      "Epoch 30/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.8817 - loss: 0.2718 - val_accuracy: 0.7940 - val_loss: 0.5282\n",
      "Epoch 31/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.8885 - loss: 0.2623 - val_accuracy: 0.8185 - val_loss: 0.4803\n",
      "Epoch 32/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 65ms/step - accuracy: 0.8932 - loss: 0.2412 - val_accuracy: 0.8185 - val_loss: 0.4649\n",
      "Epoch 33/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 61ms/step - accuracy: 0.9033 - loss: 0.2360 - val_accuracy: 0.8185 - val_loss: 0.4909\n",
      "Epoch 34/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.9006 - loss: 0.2388 - val_accuracy: 0.8165 - val_loss: 0.4762\n",
      "Epoch 35/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.9052 - loss: 0.2310 - val_accuracy: 0.8110 - val_loss: 0.4951\n",
      "Epoch 36/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 60ms/step - accuracy: 0.9045 - loss: 0.2257 - val_accuracy: 0.8070 - val_loss: 0.4956\n",
      "Epoch 37/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9154 - loss: 0.2087 - val_accuracy: 0.8155 - val_loss: 0.4735\n",
      "Epoch 38/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 49ms/step - accuracy: 0.9144 - loss: 0.2103 - val_accuracy: 0.8030 - val_loss: 0.5722\n",
      "Epoch 39/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - accuracy: 0.9243 - loss: 0.1807 - val_accuracy: 0.8035 - val_loss: 0.5126\n",
      "Epoch 40/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step - accuracy: 0.9262 - loss: 0.1847 - val_accuracy: 0.8165 - val_loss: 0.5513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14d75e960>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on Training set\n",
    "# Params: x = training set, validation = test_set\n",
    "# Epochs = trial and error\n",
    "# Batch Size = not needed.\n",
    "\n",
    "# pip install pillow for import error\n",
    "\n",
    "cnn.fit(x = training_set, validation_data= test_set, epochs = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Making Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the image, reduce size to 64 by 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image, use the specific load image function. Use Full Path.\n",
    "\n",
    "# Make sure to pass params, such as transforming the image like image reduction - 64 by 64\n",
    "image_pre = image.load_img('/Users/christine/VSCode/deeplearningcourse_remote/deeplearningcourse/Part 2 - CNN/test_set/dogs/corgi_1.jpg', target_size= (64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Use PIL Format to Numpy Array for impact, to pass into CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PIL format to an array.\n",
    "# It expects a 2D array to run the Neural Network\n",
    "test_image =image.img_to_array(image_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the preprocessing pipeline here for \"batch_size\", you need to mention it in the params:\n",
    "# batch size is the first, so axis = 0, or rows.\n",
    "test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"
     ]
    }
   ],
   "source": [
    "# Result...\n",
    "result = cnn.predict(test_image/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999982"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we use sigmoid, the output is a probaility:\n",
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index, so you don't need to memorize or encode manually what the number means in result:\n",
    "\n",
    "# This will remind you\n",
    "class_key = training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation:\n",
    "def evaluation_animal(prob):\n",
    "    if prob > 0.5:\n",
    "        print(f\"We think this is a dog.\")\n",
    "    else: \n",
    "        print(\"We think this is a cat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We think this is a dog.\n"
     ]
    }
   ],
   "source": [
    "evaluation_animal(result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Notes:\n",
    "\n",
    "* When following this code, don't forget normalization, scaling, transformation steps applied.\n",
    "* Basically, if you applied XYZ to your training set, you will likely need to do something similar to test set (but don't fit).\n",
    "* For special one-time predictions, you'll need to apply the transformations.\n",
    "* In the course, the instructor forgot to scale by 255 in the predict() section, you need to scale it or the number will always be 1.0 (dog prediction)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prevent overfit:\n",
    "\n",
    "1. Play around with parameters via hyperparameter tuning.\n",
    "2. Use Cross-Validation sets (k-folds) so the model seems more parts of the data, generalizes better.\n",
    "3. Use Regularization Techniques.\n",
    "4. Increase the training set.\n",
    "5. Try different things, like potentially a new model if it's not fitting well with your data, and you're often seeing better performance of training set over test set.\n",
    "6. Batch size\n",
    "7. Apply transformation and normalization techniques to modify your training and test data, i.e. reduce impact of outliers --> i.e. predict and divide by 255\n",
    "8. Batch Normalization - another form of regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
